{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763d39cb",
   "metadata": {},
   "source": [
    "# 4 Perceptron\n",
    "In this notebook we will want to look at the perceptron. What you have done manually in the previous task, we will now implement with Python and Numpy. Instead of choosing the weights by deliberation, first we will use the perceptron algorithm to learn the weights and later the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4548a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c8aa0",
   "metadata": {},
   "source": [
    "## 4.0 Recap\n",
    "We need to implement three major parts for our objective: a data set, the foward pass and the backward pass.\n",
    "\n",
    "Remember that our perceptron has two inputs, a bias and three learnable weights.\n",
    "\n",
    "![Perceptron](./fig/PerceptronG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254345a",
   "metadata": {},
   "source": [
    "## 4.1 The Data\n",
    "Our aim is to learn the boolean function AND ($\\land$). For that purpose we can create our own data with the corresponding target/labels. We will use 0 for False and 1 for True.\n",
    "Also, the variable that holds the data will be denominated with _x_ and the variable holding the labels will be _t_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3eaed",
   "metadata": {},
   "source": [
    "### Task 4.1.1 Create the data\n",
    "We will need a dataset that holds all possible inputs for the Boolean function AND. Here is a quick reminder on how that function works:\n",
    "\n",
    "| A| B | A AND B  |\n",
    "| :--- | :--- | --- |\n",
    "| True | True | True |\n",
    "| True | False | False |\n",
    "| False | True | False |\n",
    "| False | False | False |\n",
    "\n",
    "Our dataset needs to contain all four possible combinations of True and False. As we have a bias, we can treat it just like an input neuron with a fixed input of True.\n",
    "\n",
    "The labels should contain the teaching signal or ground truth for each dataset sample (as a vector), i.e. the correct output of the corresponding input.\n",
    "\n",
    "Both, input data and labels, should be represented as Numpy Matrices/Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f7f4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.,1.,1.], [1.,1.,0.], [1.,0.,1.], [1.,0.,0.]])\n",
    "# note to avoid confusion: the first '1' in each array is the fixed input of True for the bias, as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dbedf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1.],[0.],[0.],[0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80fe23",
   "metadata": {},
   "source": [
    "## 4.2 The Forward Pass\n",
    "Now we have to setup all necessary functions to complete a first forward pass through our perceptron and obtain a prediction based on the input.\n",
    "We need to be able to calculate the inner activation of the neuron _h_, define the activation function _g_, and finally calculate the outer activation _y_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f983117",
   "metadata": {},
   "source": [
    "### Task 4.2.1 Inner activation _h_\n",
    "From the lecture we know that the inner activation _h_ is just a weighted sum of the inputs.\n",
    "\n",
    "$$  \\large h_i = \\sum_{j=1}^{n} w_{ij} x_j $$\n",
    "\n",
    "Complete the function `inner_activation(...)`. As inputs it should take the input activations and the synaptic weights and return inner activation _h_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c5573257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 2., 1.])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows of weight matrix correspond to the individual neurons in the same layer\n",
    "# each value in a row corresponds to an input neuron from the previous layer\n",
    "def inner_activation(weights, inputs):\n",
    "    layer_len, prev_layer_len = weights.shape\n",
    "    outputs = []\n",
    "    for n in range(0, layer_len):\n",
    "        output = 0\n",
    "        for w, i in zip(weights[n], inputs):\n",
    "            output += w*i\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "inner_activation(\n",
    "    np.array([[1., 2.], \n",
    "              [0., 1.], \n",
    "              [1., 0.]]),\n",
    "    np.array( [1., 2.])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e5ba6",
   "metadata": {},
   "source": [
    "### Task 4.2.2 The activation function _g_\n",
    "The perceptron algorithm uses a step function as its activation function (sometimes also refered to as Heaviside function), to transfer the inner activation _h_ to the outer activation _y_. It looks as follows:\n",
    "\n",
    "$$    y_i = \n",
    "    \\begin{cases}\n",
    "      1 & \\text{if } h_i \\geq \\theta \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases} $$\n",
    "    \n",
    "As the threshold, we define $\\theta$ to be 0.\n",
    "\n",
    "Complete the function `g(...)`. It should take the inner activation _h_ as an input and return the outer activation _y_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e0faaade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(inner_activation):\n",
    "    if inner_activation >= 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0d226",
   "metadata": {},
   "source": [
    "### Task 4.2.3 Plot the activation function\n",
    "In order to check if we have done it correctly, we can plot the activation function.\n",
    "Create artificial datapoints in the range of -1 to 1, feed them into the activation function and plot the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c683f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"activation function\")\n",
    "plt.xlabel(\"inner activation\")\n",
    "plt.ylabel(\"outer activation\")\n",
    "r = map(lambda v: v / 10, range(-10, 11))\n",
    "rl = list(r)\n",
    "plt.plot(rl, list(map(g, rl)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983efaf",
   "metadata": {},
   "source": [
    "### Task 4.2.4 Putting it together\n",
    "Now we can combine our functions to compute the whole forward pass of the perceptron.\n",
    "\n",
    "Compete the function `forward_pass(...)`, which takes the input activations and the weights as inputs and returns the outer activation _y_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "98945fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_pass(weights, inputs):\n",
    "    outputs = inner_activation(weights, inputs)\n",
    "    return np.array(list(map(g,  outputs)))\n",
    "\n",
    "forward_pass(\n",
    "    np.array([[1., 2.], \n",
    "              [0., 1.], \n",
    "              [1., 0.]]),\n",
    "    np.array( [1., 2.])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5da459",
   "metadata": {},
   "source": [
    "## 4.3 The Backward Pass\n",
    "The backward pass changes the weights in respect to the error.\n",
    "\n",
    "From the lecture we know that the update rule of the perceptron algorithm looks like this:\n",
    "\n",
    "$$ \\large \\Delta w_{ij} = \\eta (t_i - y_i) x_j $$\n",
    "$$ \\large w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij} $$\n",
    "\n",
    "$\\eta$ is a hyperparameter that needs to be set. In our case, 0.01 is a reasonable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6121eb",
   "metadata": {},
   "source": [
    "### Task 4.3.1 Update the weights\n",
    "Complete the function `update(...)` which takes as input the current weights, the label, the input activation and the outer activation and returns the updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9487cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights are implicitly returned as the parameter object is modified directly\n",
    "def update(weights, inputs, outputs, labels):\n",
    "    ETA = 0.01/1.0 # 10% change for an error of 1\n",
    "    layer_len, prev_layer_len = weights.shape\n",
    "    for ineuron in range(layer_len):\n",
    "        error = labels[ineuron] - outputs[ineuron]\n",
    "        for iinput in range(prev_layer_len):\n",
    "            delta = error * ETA * inputs[iinput]\n",
    "            weights[ineuron, iinput] += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6878a54",
   "metadata": {},
   "source": [
    "## 4.4 Learning the boolean function\n",
    "Now we can use all the functions we have written, to piece together the perceptron algorithm and learn the AND function. Before we start, we need to set a start point in the weight space. For that purpose we define the weights more or less randomly before the learning starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d17db9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[3.0,-2.0,-2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b779136",
   "metadata": {},
   "source": [
    "### Task 4.4.1 The training loop\n",
    "In neural network training, you will often encouter a central training loop that iterates through the dataset and updates the weights regularly according to the generated predictions.\n",
    "Your task now is to write this training loop.\n",
    "\n",
    "For one epoch, step through all datapoints:\n",
    "  - compute the outer activation for the datapoint\n",
    "  - update the weights accordingly.\n",
    "  \n",
    "Do this for 1000 epochs.\n",
    "Every 50 epochs, print the current weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d25dba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights in epoch  000 : [[ 3. -2. -2.]]\n",
      "Weights in epoch  050 : [[ 2. -2. -2.]]\n",
      "Weights in epoch  100 : [[ 1.65 -1.68 -1.67]]\n",
      "Weights in epoch  150 : [[ 1.32 -1.34 -1.34]]\n",
      "Weights in epoch  200 : [[ 0.98 -1.01 -1.01]]\n",
      "Weights in epoch  250 : [[ 0.65 -0.68 -0.67]]\n",
      "Weights in epoch  300 : [[ 0.32 -0.34 -0.34]]\n",
      "Weights in epoch  350 : [[-0.01 -0.01 -0.01]]\n",
      "Weights in epoch  400 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  450 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  500 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  550 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  600 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  650 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  700 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  750 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  800 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  850 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  900 : [[-0.03  0.01  0.02]]\n",
      "Weights in epoch  950 : [[-0.03  0.01  0.02]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[3.0,-2.0,-2.0]])\n",
    "for epoch in range(1000):\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Weights in epoch \", \"%03d\" % (epoch,), \":\", w)\n",
    "    for inputs, labels in zip(x, t):\n",
    "        outputs = forward_pass(w, inputs)\n",
    "        #print(\"inputs\", end=\"\")\n",
    "        #print(inputs, end=\"\")\n",
    "        #print(\"; outputs\", end=\"\")\n",
    "        #print(outputs, end=\"\")\n",
    "        #print(\"; labels\", end=\"\")\n",
    "        #print(labels)\n",
    "        update(w, inputs, outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6188894",
   "metadata": {},
   "source": [
    "As you might have realised, the weights do not change anymore after a certain point. The perceptron algorithm has converged.\n",
    "We now have a set of weights, that we can use for predicting Boolean values. But before we deploy our model, we need to evaluate its quality. Has it really learned the function properly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790f568",
   "metadata": {},
   "source": [
    "### Task 4.4.2 The error _E(w)_\n",
    "In order to evaluate how good our current set of weights _w_ is, we can compute the error.\n",
    "\n",
    "$$ \\large E(w) = \\frac{1}{2} \\sum_i (t_i - y_i)^2 $$\n",
    "\n",
    "Implement the function `error(...)` which takes predcitions (outer activations) and (labels) as input and returns the error.\n",
    "\n",
    "Afterwards, compute the prediction error for the whole dataset. If it is zero, your model predicts perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e5701db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(predictions, labels):\n",
    "    return 0.5 * sum((t-y)**2 for t, y in zip(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ab566027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# getting the prediction error for the whole dataset\n",
    "preds = [forward_pass(w, inputs) for inputs in x]\n",
    "print(error(t,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcc40",
   "metadata": {},
   "source": [
    "## 4.5 Deploy the model\n",
    "Now that we have a perfectly working and evaluated model, the most interesting part for the machine learning researcher is done. However, the end user, who might not be as familiar with neural networks as we are, might have trouble applying the model to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fcf1e",
   "metadata": {},
   "source": [
    "### Task 4.5.1 The AND function\n",
    "We want to write a function that does not require the user to know about inner and outer activations or transfer functions but still enable them to harness the power of our model.\n",
    "\n",
    "Complete the function `AND(...)` that takes as input too boolean values A and B and returns the prediction of our perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0e8895d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def AND(A, B):\n",
    "    return True if forward_pass(w, [1.0, 1.0 if A else 0.0, 1.0 if B else 0.0])[0] > 0.5 else False\n",
    "print(AND(True, True))\n",
    "print(AND(True, False))\n",
    "print(AND(False, True))\n",
    "print(AND(False, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
